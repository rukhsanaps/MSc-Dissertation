{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e5492f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58bd7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50868683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from keras_tuner.tuners import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69dc7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "660adaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   current_value  lights         T1       RH_1         T2       RH_2  \\\n",
      "0          430.0      30  20.133333  48.000000  19.566667  44.400000   \n",
      "1          250.0      30  20.260000  52.726667  19.730000  45.100000   \n",
      "2          100.0      10  20.426667  55.893333  19.856667  45.833333   \n",
      "3          100.0      10  20.566667  53.893333  20.033333  46.756667   \n",
      "4           90.0      10  20.730000  52.660000  20.166667  47.223333   \n",
      "\n",
      "          T3       RH_3         T4       RH_4  ...  is_weekend    nsm  lag_1  \\\n",
      "0  19.890000  44.900000  19.000000  46.363333  ...           0  68400  576.6   \n",
      "1  19.890000  45.493333  19.000000  47.223333  ...           0  69000  430.0   \n",
      "2  20.033333  47.526667  19.000000  48.696667  ...           0  69600  250.0   \n",
      "3  20.100000  48.466667  19.000000  48.490000  ...           0  70200  100.0   \n",
      "4  20.200000  48.530000  18.926667  48.156667  ...           0  70800  100.0   \n",
      "\n",
      "   lag_2  lag_6  lag_12  hour_sin  hour_cos  rolling_mean_12  rolling_std_12  \n",
      "0  230.0   60.0    60.0 -0.965926  0.258819       115.550000      153.488504  \n",
      "1  576.6   60.0    60.0 -0.965926  0.258819       146.383333      176.720271  \n",
      "2  430.0   60.0    50.0 -0.965926  0.258819       162.216667      176.788676  \n",
      "3  250.0   70.0    50.0 -0.965926  0.258819       166.383333      174.477557  \n",
      "4  100.0  230.0    60.0 -0.965926  0.258819       170.550000      172.025350  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the data into dataframe\n",
    "df=pd.read_csv('feature_engineered_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa039dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_model(df,selected_features,mins):\n",
    "    step=mins//10\n",
    "\n",
    "    df['target']= df['current_value'].shift(-step)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    df = df[selected_features + ['target']]\n",
    "\n",
    "    X=df.drop(columns=['target'])\n",
    "    y=df['target']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split (chronologically for time series)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4)\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on training and test\n",
    "    xgb_train_preds = xgb.predict(X_train)\n",
    "    xgb_test_preds = xgb.predict(X_test)\n",
    "\n",
    "    # Stack original inputs + xgb prediction\n",
    "    X_train_lstm = np.hstack((X_train, xgb_train_preds.reshape(-1, 1)))\n",
    "    X_test_lstm = np.hstack((X_test, xgb_test_preds.reshape(-1, 1)))\n",
    "\n",
    "    # Reshape for LSTM: (samples, timesteps, features)\n",
    "    X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], 1, X_train_lstm.shape[1]))\n",
    "    X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], 1, X_test_lstm.shape[1]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train_lstm, y_train, epochs=20, batch_size=64, validation_split=0.1, verbose=1,shuffle=False)\n",
    "\n",
    "    y_pred = model.predict(X_test_lstm)\n",
    "\n",
    "# Evaluation\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Minutes:{mins} :- RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2760f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['current_value', 'RH_5', 'T6', 'T8', 'RH_8', 'Press_mm_hg', 'nsm', 'lag_1', 'rolling_mean_12', 'rolling_std_12']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rukhsana\\anaconda3\\envs\\project\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 12616.1992 - mae: 66.3328 - val_loss: 2169.4253 - val_mae: 21.5339\n",
      "Epoch 2/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3288.4795 - mae: 29.1248 - val_loss: 2164.6760 - val_mae: 21.5669\n",
      "Epoch 3/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3262.6765 - mae: 29.0421 - val_loss: 2165.3875 - val_mae: 21.4673\n",
      "Epoch 4/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3277.0188 - mae: 29.1719 - val_loss: 2159.9885 - val_mae: 21.5159\n",
      "Epoch 5/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3228.5696 - mae: 28.7276 - val_loss: 2156.6033 - val_mae: 21.5890\n",
      "Epoch 6/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3273.5906 - mae: 29.0696 - val_loss: 2156.5088 - val_mae: 21.4781\n",
      "Epoch 7/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3212.6960 - mae: 28.9469 - val_loss: 2152.7554 - val_mae: 21.3122\n",
      "Epoch 8/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3166.2686 - mae: 28.5797 - val_loss: 2151.1970 - val_mae: 21.6853\n",
      "Epoch 9/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3220.4788 - mae: 28.7607 - val_loss: 2154.5002 - val_mae: 21.1897\n",
      "Epoch 10/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3213.1001 - mae: 28.8647 - val_loss: 2147.5886 - val_mae: 21.3260\n",
      "Epoch 11/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3181.5034 - mae: 28.6962 - val_loss: 2153.4761 - val_mae: 21.0521\n",
      "Epoch 12/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3182.9902 - mae: 28.6459 - val_loss: 2143.8901 - val_mae: 21.2828\n",
      "Epoch 13/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3134.4353 - mae: 28.3745 - val_loss: 2144.6323 - val_mae: 21.2316\n",
      "Epoch 14/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3225.4187 - mae: 28.9827 - val_loss: 2142.9307 - val_mae: 21.3437\n",
      "Epoch 15/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3205.9395 - mae: 28.7374 - val_loss: 2142.1484 - val_mae: 21.2099\n",
      "Epoch 16/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3186.1541 - mae: 28.4718 - val_loss: 2139.2307 - val_mae: 21.3585\n",
      "Epoch 17/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3176.5300 - mae: 28.6089 - val_loss: 2145.2695 - val_mae: 20.9822\n",
      "Epoch 18/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3122.9314 - mae: 28.4099 - val_loss: 2136.3003 - val_mae: 21.2534\n",
      "Epoch 19/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3177.6587 - mae: 28.5788 - val_loss: 2136.6418 - val_mae: 21.1149\n",
      "Epoch 20/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3201.6479 - mae: 28.8012 - val_loss: 2133.2263 - val_mae: 21.2654\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Minutes:10 :- RMSE: 55.77, MAE: 25.15, R²: 0.5883\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the selected features for 10-minute forecasting\n",
    "with open(\"selected_features_10min.pkl\", \"rb\") as f:\n",
    "    selected_features_10 = pickle.load(f)\n",
    "\n",
    "print(\"Selected Features:\", selected_features_10)\n",
    "\n",
    "implement_model(df,selected_features_10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b784c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['current_value', 'T3', 'RH_5', 'RH_8', 'T_out', 'Press_mm_hg', 'nsm', 'hour_cos', 'rolling_mean_12', 'rolling_std_12']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rukhsana\\anaconda3\\envs\\project\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14268.6934 - mae: 70.0530 - val_loss: 4471.5249 - val_mae: 35.2350\n",
      "Epoch 2/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6087.2612 - mae: 45.2242 - val_loss: 4436.9380 - val_mae: 34.0956\n",
      "Epoch 3/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6025.2886 - mae: 44.4981 - val_loss: 4458.3867 - val_mae: 33.4747\n",
      "Epoch 4/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6006.0586 - mae: 44.2724 - val_loss: 4413.8379 - val_mae: 33.6329\n",
      "Epoch 5/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6019.4824 - mae: 44.3105 - val_loss: 4400.0312 - val_mae: 33.5870\n",
      "Epoch 6/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5966.8931 - mae: 43.7464 - val_loss: 4374.5293 - val_mae: 33.8256\n",
      "Epoch 7/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5924.3149 - mae: 43.7616 - val_loss: 4380.2939 - val_mae: 33.4011\n",
      "Epoch 8/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5875.0337 - mae: 43.4991 - val_loss: 4400.3540 - val_mae: 33.0943\n",
      "Epoch 9/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5946.3984 - mae: 43.7306 - val_loss: 4360.4814 - val_mae: 33.0426\n",
      "Epoch 10/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5872.3545 - mae: 43.3112 - val_loss: 4344.6353 - val_mae: 32.8417\n",
      "Epoch 11/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5895.6343 - mae: 43.2647 - val_loss: 4339.7388 - val_mae: 32.6700\n",
      "Epoch 12/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5828.3608 - mae: 43.0854 - val_loss: 4320.3779 - val_mae: 32.7341\n",
      "Epoch 13/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5878.2705 - mae: 43.1389 - val_loss: 4335.7002 - val_mae: 32.5822\n",
      "Epoch 14/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5845.8711 - mae: 42.9588 - val_loss: 4305.3853 - val_mae: 32.6820\n",
      "Epoch 15/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5857.7031 - mae: 43.1135 - val_loss: 4272.9404 - val_mae: 32.6742\n",
      "Epoch 16/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5726.3433 - mae: 42.8053 - val_loss: 4299.0742 - val_mae: 32.5234\n",
      "Epoch 17/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5890.4048 - mae: 43.0788 - val_loss: 4372.5610 - val_mae: 32.2211\n",
      "Epoch 18/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5878.5591 - mae: 42.7154 - val_loss: 4273.6821 - val_mae: 32.5186\n",
      "Epoch 19/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5773.4009 - mae: 42.8289 - val_loss: 4289.0845 - val_mae: 32.4287\n",
      "Epoch 20/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5814.9917 - mae: 42.6784 - val_loss: 4299.3286 - val_mae: 32.2985\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Minutes:60 :- RMSE: 79.79, MAE: 44.96, R²: 0.1562\n"
     ]
    }
   ],
   "source": [
    "with open(\"selected_features_60min.pkl\", \"rb\") as f:\n",
    "    selected_features_60 = pickle.load(f)\n",
    "\n",
    "print(\"Selected Features:\", selected_features_60)\n",
    "\n",
    "implement_model(df,selected_features_60,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43ab2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(df,selected_features,mins):\n",
    "    step=mins//10\n",
    "\n",
    "    df['target']= df['current_value'].shift(-step)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    df = df[selected_features + ['target']]\n",
    "\n",
    "    X=df.drop(columns=['target'])\n",
    "    y=df['target']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "    #y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    # Train-test split (chronologically for time series)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "    y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "    \n",
    "\n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(50, 200),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 0.3, 'log-uniform'),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0)\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        XGBRegressor(),\n",
    "        search_spaces,\n",
    "        n_iter=20,\n",
    "        scoring='r2',\n",
    "        cv=tscv,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "    best_xgb = opt.best_estimator_\n",
    "\n",
    "    # Get XGB predictions\n",
    "    xgb_train_preds = best_xgb.predict(X_train).reshape(-1, 1)\n",
    "    xgb_test_preds = best_xgb.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    # Combine XGB predictions with original features\n",
    "    X_train_gru = np.hstack((X_train, xgb_train_preds))\n",
    "    X_test_gru = np.hstack((X_test, xgb_test_preds))\n",
    "\n",
    "    # Reshape for GRU (samples, timesteps=1, features)\n",
    "    X_train_gru = X_train_gru.reshape((X_train_gru.shape[0], 1, X_train_gru.shape[1]))\n",
    "    X_test_gru = X_test_gru.reshape((X_test_gru.shape[0], 1, X_test_gru.shape[1]))\n",
    "\n",
    "    def build_gru_model(hp):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(\n",
    "            units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
    "            activation='tanh',\n",
    "            input_shape=(X_train_gru.shape[1], X_train_gru.shape[2])\n",
    "        ))\n",
    "        model.add(Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    tuner = BayesianOptimization(\n",
    "        build_gru_model,\n",
    "        objective='val_mae',\n",
    "        max_trials=15,\n",
    "        directory='xgboost_gru_tuning',\n",
    "        project_name=f'xgb_gru{mins}'\n",
    "    )\n",
    "\n",
    "    tuner.search(X_train_gru, y_train, epochs=20, validation_split=0.2, batch_size=64, verbose=1,shuffle=False)\n",
    "\n",
    "    # Get best model\n",
    "    best_gru_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    y_pred = best_gru_model.predict(X_test_gru)\n",
    "\n",
    "    y_pred_actual = target_scaler.inverse_transform(y_pred)\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_actual))\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "    r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "\n",
    "    print(f\" XGB-GRU {mins} mins:- RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68e428e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 12s]\n",
      "val_mae: 0.046236272901296616\n",
      "\n",
      "Best val_mae So Far: 0.042810313403606415\n",
      "Total elapsed time: 00h 02m 44s\n",
      "\u001b[1m 74/124\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 767us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rukhsana\\anaconda3\\envs\\project\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      " XGB-GRU 10 mins:- RMSE: 55.43, MAE: 24.92, R²: 0.5933\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning(df,selected_features_10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cbc9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 10s]\n",
      "val_mae: 0.0764235407114029\n",
      "\n",
      "Best val_mae So Far: 0.07605809718370438\n",
      "Total elapsed time: 00h 02m 43s\n",
      "\u001b[1m 67/124\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 780us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rukhsana\\anaconda3\\envs\\project\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      " XGB-GRU 60 mins:- RMSE: 76.32, MAE: 42.30, R²: 0.2281\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tuning(df,selected_features_60,60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
